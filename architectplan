Architectural Plan for ASML Django-Based Application

1. High-Level Architecture Overview

 Figure: High-level Django web application architecture.
At a high level, the application will be a web-based system built on the Django framework (Python) with a MySQL database backend. The architecture follows a typical web application model: client requests hit a web server (e.g. Nginx) which proxies to the Django application, and Django interacts with the MySQL database for data persistence. Django is chosen because it is a powerful web framework that includes convenient features like an ORM for database access, built-in authentication, and an administrative interface – all of which accelerate development. This stack is proven in production; for example, large-scale sites (like parts of the Washington Post) have successfully used Django, demonstrating that it can scale to thousands or even millions of users when designed well. Key aspects of the high-level design include:

Modular Django apps for different domains (reservations, allocations, etc.), promoting separation of concerns and maintainability.

Role-based dashboards and views that are dynamically configurable based on user roles.

Okta integration for single sign-on (SSO) authentication and enterprise-grade security.

A scalable architecture that can handle thousands of users concurrently through horizontal scaling and efficient use of caching.

Flexible, configurable forms and workflows that can adapt to different department needs without code changes.


2. Handling Multiple Platforms and Roles

The application will support multiple user roles and slight variations in workflow per department through a robust role-based access control design. We will leverage Django’s built-in authentication and permissions framework (users, groups, and permissions) rather than inventing a custom solution. This approach is recommended because Django’s permission system is flexible and avoids reimplementing features that already exist. On top of Django’s permissions, we will implement a Role-Based Access Control (RBAC) model: each user is assigned one or more roles (e.g. engineer, planner, admin), and these roles determine what the user can see and do in the system. Permissions will be enforced at the view level and in templates, so that each role only accesses the appropriate dashboards and actions.

To accommodate department-specific workflows without duplicating code, the workflow configuration will be data-driven. We will introduce a configurable settings model in the database to define things like form fields, approval steps, or notifications for each department. This means the workflow steps aren’t hard-coded in Python; instead, they can be adjusted via database entries if a department needs a minor tweak. This approach aligns with the idea of not hard-coding workflows but making them configurable in the database (SQL layer). For example, the sequence of status changes for a test reservation could be stored in a configuration table, and different departments can have variations of those sequences. By doing this, we achieve flexibility—adjusting a workflow later won’t require a code deployment, just a data change.

3. Application Structure

The project will be organized into multiple Django apps, each encapsulating a distinct piece of functionality. Using separate apps for logically separate concerns is a Django best practice for modular architecture. This ensures each component is cohesive and can potentially be developed or tested in isolation. The planned apps and their responsibilities are:

Reservations App – Manages test reservations and their lifecycle. This includes creating reservation requests, tracking status (e.g. requested, approved, completed), and enforcing the workflow rules for testing procedures. (This app essentially replaces and enhances the current “Proto Order” CGI forms, providing a more guided workflow and UI for reservations.)

Allocations App – Handles scheduling of personnel on machines and test benches. It will likely provide a calendar or drag-and-drop interface for planners to assign engineers/technicians to machines for specific time slots. This ensures that resource allocation is visible and avoid conflicts.

User Management App – Centralizes authentication and user profiles/roles. This app integrates with Okta for SSO and manages local user data (like role assignments, profile info). Role-based access control logic (groups/permissions) will reside here so that other apps can query it to decide access.

Integration App – Prepares for future integrations, notably with Microsoft Project or other scheduling tools. Initially this might contain just placeholder code or basic API endpoints, but it’s structured to accommodate features like importing project timelines or exporting data.

Proto Logbook App – Acts as an activity log or journal of all actions on prototypes and reservations. Every significant event (reservation created, status changed, test result logged, etc.) will generate an entry. This provides an audit trail and a historical logbook that engineers and managers can review for any prototype’s history.


Each app is a self-contained Django module with its own models, views, and templates, which keeps the codebase organized. This modular approach means, for example, that changes to the reservations workflow don’t affect the allocation scheduling component, and vice versa. It also makes it easier to assign different teams to different parts of the application if needed.

4. Database Design & Migration Strategy

The existing MySQL database schema from the legacy system will be brought into Django with careful planning. First, we will assess the current schema to understand tables, relationships, and data. Django’s Object-Relational Mapper will be introduced on top of this schema by defining Django models corresponding to the existing tables. We can automate part of this with Django’s inspectdb utility, which introspects an existing database and generates model definitions. This gives us a starting point for models that match the current tables. Those models will be adjusted (if necessary) to fit Django conventions while maintaining compatibility with the existing data – for instance, preserving primary key fields or important constraints so we can seamlessly use the current data.

Once the models are in place, we will use Django’s migration framework for schema evolution going forward. Migrations in Django act like version control for the database schema, propagating model changes (new fields, tables, etc.) to the database in a controlled manner. We will create an initial migration that reflects the current schema (so Django knows the baseline). Future changes (such as adding new tables for additional features, or altering fields for new requirements) will be done through Django migrations, ensuring that changes are applied consistently across development, testing, and production databases.

For moving the historical data, we’ll perform an ETL (Extract, Transform, Load) process if needed. The goal is to retain all relevant data (past reservations, user accounts, etc.). If the schema changes (for example, splitting a single legacy table into multiple tables in Django for normalization), we will write scripts to migrate data accordingly and verify consistency. A comprehensive testing strategy will be used to validate the data migration: before going live, we’ll run the migration in a staging environment and cross-check record counts, sample entries, and system behavior against the old system to ensure nothing was lost or corrupted. Only after confirming the integrity in staging will we perform the final migration on production data.

5. Migration Strategy from Perl to Django

Migrating from the Perl CGI application to the new Django application will be done in phases to minimize risk and downtime. We will not attempt a “big bang” cutover; instead, the plan is to gradually replace parts of the old system with the new one, ensuring at each step that the application remains functional.

Phase 1: Read-Only & Parallel Running. In this initial phase, we deploy the Django application in parallel with the Perl system, but perhaps in a read-only or pilot mode. For example, the Django app could connect to the existing database and allow users to view data (reservations, schedules) through the new interface, without yet allowing writes. This lets us test the waters — verifying that the Django app is correctly reading data and rendering it — while all writes (new reservations, updates) still happen through the old Perl interface. We can have a subset of users start using the new interface for viewing or double-entering data to test it.

Phase 2: Gradual Cutover of Write Operations. Once confidence is gained in the Django app’s correctness, we begin routing write operations to Django. This could be done module by module or user group by user group. For instance, we might switch the reservations functionality to Django while still doing allocations in the old system, or vice-versa. During this phase, both systems might be live, but users will increasingly perform actions in Django. We will implement data synchronization or ensure Django and Perl are using the same database so that whichever interface is used, the underlying data is consistent. Throughout this phase, we keep comprehensive backups and closely monitor for any issues.

Phase 3: Full Deprecation of Perl System. After all major workflows (reservations, allocations, etc.) are running smoothly on Django and all users have been onboarded to the new system, we will retire the Perl CGI application. The final step is to turn off the old interface, leaving the Django app as the sole system. At this point, any remaining background scripts or batch jobs that were running in Perl will also be reimplemented (perhaps as Django management commands or Celery tasks). We’ll ensure that features like the automated email reminders and summaries are ported over to the new system’s stack. When the Perl system is fully turned off, we will have Django exclusively managing the platform.


Throughout these phases, thorough testing and user training are crucial. We will create test scripts to compare outputs from both systems for the same inputs, and we’ll likely run both systems in parallel for a time to ensure nothing is missed. The phased approach ensures that at no point we lose functionality and that we can roll back or fix forward quickly if any issues arise in the new system.

6. Deployment Strategy

The deployment environment for the new application will be modern and automated. We plan to deploy on a Unix/Red Hat Linux server environment, using Docker containers for the application. Containerizing the Django app (and related services like MySQL, if needed) provides consistency between development and production and makes it easier to manage dependencies. The containers will be orchestrated with Kubernetes (K8s), which will handle scaling the application pods, self-healing (restarting any crashed containers), and rolling updates for deployments.

In front of the Django app, we will use Nginx as a reverse proxy and static file server. Nginx is a popular choice for Django deployments and is well-proven in terms of performance and reliability. It will serve several purposes: terminating SSL (HTTPS) connections, serving static/media files directly, and forwarding application requests to the Django application (which will likely run via Gunicorn WSGI servers inside the containers). This setup (Nginx + Gunicorn + Django) is a standard production configuration for Django apps.

We will establish a new CI/CD pipeline for automated testing and deployment. When code is pushed to the repository, automated tests will run (including unit tests and perhaps integration tests using Django’s test framework). Upon success, the CI system can build a new Docker image for the app. Deployment to Kubernetes can then be orchestrated, ensuring zero-downtime updates (using rolling updates). Configuration for different environments (dev, staging, prod) will be managed via environment variables and Kubernetes secrets/configMaps, rather than hardcoding anything.

Additionally, we’ll integrate logging and monitoring from day one. Application logs (from Django and Gunicorn) will be aggregated (for example, using ELK stack or a cloud logging service) to track errors and usage. Monitoring tools will be in place to watch the health of the application – this could include uptime monitoring, metrics (like response times, query counts, CPU/memory of containers), and alerts if anything goes out of expected bounds. This way, we can catch performance issues or errors early.

By containerizing the app and using Kubernetes, the deployment becomes cloud-friendly and scalable. If usage grows, we can increase the number of Django container replicas to handle more load. If we need maintenance, we can replace containers one at a time without downtime. The deployment strategy thus emphasizes reliability, scalability, and maintainability.

7. Security Considerations

Security is a top priority, especially given the enterprise context. Authentication will be handled via Okta, which provides a secure single sign-on solution. Users will log in through Okta, which manages credentials and multi-factor authentication, and the Django app will trust Okta as an identity provider. Okta is an identity management platform that supports protocols like OAuth2/OIDC and SAML, allowing it to seamlessly integrate with web applications for SSO. This means our app won’t store plaintext passwords or handle login flows directly – those are offloaded to Okta, which is thoroughly secure and compliance-certified. Django will receive an assertion or token from Okta that confirms the user’s identity, and we’ll map that to a Django user account and roles internally.

Within the Django app, we will enforce role-based permissions at every layer. Django’s authentication system and our RBAC logic will ensure that each view checks the user’s role/permissions. We’ll use Django’s decorators (like @login_required and permission checks) on views so that nothing sensitive is accessible without proper auth. In templates and APIs, we’ll conditionally show or enable actions based on the user’s permissions. By using Django’s built-in framework for this, we reduce the chance of oversight, since Django’s framework is well-tested.

All communication with the app will be over HTTPS. We will obtain and configure TLS certificates (potentially using Let’s Encrypt or the company’s certificate authority) so that data in transit is encrypted. We will also enable secure cookies (setting the SESSION_COOKIE_SECURE and CSRF_COOKIE_SECURE settings in Django) so that cookies are not sent over non-HTTPS connections, and HttpOnly flags to prevent JavaScript access to session cookies. Django has a CSRF protection middleware enabled by default, which will help protect against cross-site request forgery attacks. We’ll ensure any forms or state-changing requests in the app use the CSRF tokens properly (Django makes this straightforward with its template tag and middleware).

Other security best practices will be followed: for instance, using Django’s SecurityMiddleware (which adds several security headers like Content Security Policy, XSS protection, etc.), keeping all software up to date with security patches, and performing regular security audits. The system will undergo penetration testing (possibly by ASML’s internal security team or third-party auditors) to find any vulnerabilities. We will also leverage Okta’s security features (like account lockout, MFA, etc.) to protect user accounts. By combining Okta’s secure authentication with Django’s in-built security measures and our own vigilance, the application will meet high security standards.

8. Handling Scalability

To ensure the application performs well as load increases, we incorporate scalability considerations from the start. One major aspect is caching. Django comes with a robust caching framework that lets us store dynamic content so it doesn’t need to be recalculated on every request. We will set up a Redis cache server and use it as the backend for Django’s cache. This can cache expensive queries or pages—for example, the dashboard data that many users see frequently could be cached for a short interval, drastically reducing database load. Additionally, sessions could be stored in Redis if needed, and we might cache template fragments or computed results (like a complex report) to speed up response times.

The database will be tuned for performance as well. We will analyze query patterns and add indexes to MySQL tables where appropriate to optimize lookups. If we anticipate heavy read traffic, we can set up read replicas of the MySQL database – the Django ORM supports directing read operations to replicas and writes to a master, in a configuration known as database replication. This way, multiple database servers can share the read load. We’ll also ensure efficient use of connections (possibly using connection pooling) and consider the isolation levels or other MySQL settings to balance consistency with performance needs.

For scaling the application layer, since we are deploying via Kubernetes, horizontal scaling is straightforward. We can increase the number of Django/Gunicorn pods to handle more simultaneous requests. The stateless nature of Django (each request independent) means we can run many instances behind a load balancer to distribute user traffic. This is a key reason why Django can scale horizontally – as long as the app is stateless (which it will be, aside from the database and cache), we can add more processes to handle more users.

We will also implement background task processing for any long-running or heavy computations. Using Celery (with a message broker like Redis), the app can offload tasks that don’t need to block the web request/response cycle. Celery is a distributed task queue that runs tasks asynchronously, preventing delays in user-facing requests. For example, generating a large report or sending a batch of emails will be done in the background. Users can trigger the action, and Celery will handle it while the user continues with other work. This keeps the web interface snappy. Celery will also be used for scheduling periodic tasks (e.g. daily summary emails or cleanup jobs) via Celery Beat. By decoupling such tasks from the main app, we improve responsiveness and throughput.

Overall, these strategies (caching, database optimizations, horizontal scaling, background processing) ensure that the system can scale to a large number of users and requests. We will also do load testing (using tools to simulate high traffic) to find any bottlenecks and address them—whether that means further optimizing code or scaling out more infrastructure. The architecture is cloud-ready, so provisioning additional resources should be straightforward if needed.

9. Future Considerations

Looking ahead, we want the platform to be flexible for future needs. One anticipated extension is a more modern front-end. While initially the Django app will render server-side HTML templates (possibly using Django’s built-in template system for the dashboards and forms), we are open to integrating a front-end framework like React or Angular for a richer UI. In the future, we might develop a separate React single-page application that communicates with Django via APIs. Django would then act primarily as a RESTful API server (using Django REST Framework) and serve the React front-end. This kind of decoupled architecture is common and combines Django’s robust backend with React’s dynamic user interface capabilities. It would allow, for instance, more interactive scheduling interfaces or real-time updates on the UI via AJAX/WebSockets. We have structured the backend in a way (with clear APIs in mind) that such a front-end could be added with minimal friction when the time comes.

We also plan to implement APIs for Microsoft Project integration. This could involve the Django app either exporting data in a format that Microsoft Project can import, or calling Microsoft Project’s APIs to pull schedule information. Microsoft Project (especially the online or server versions) has APIs and integration points (possibly through Microsoft Graph or other endpoints). Down the line, we could automate the exchange of scheduling data – for example, if project managers maintain a timeline in MS Project, the integration app could fetch relevant milestones or resource assignments and update the Django app’s allocation schedule automatically. This would eliminate double entry and keep the data in sync with corporate project plans.

Another future area is enhanced reporting and analytics. With all reservations and test data in Django, we can leverage Python’s ecosystem for data analysis or use a tool like Tableau or Power BI via database connection. We might build an reporting module that offers charts or KPIs about lab usage, test throughput, etc. The architecture will allow adding such features, either by new Django apps or external tools connecting to the same database.

Finally, maintainability and extensibility will remain important. We will keep the codebase clean, well-documented, and write automated tests for new features. This makes it easier to onboard new developers and adapt to new requirements. The idea is that this Django-based system will serve ASML’s needs for years to come, growing and evolving as needed.

In summary, this plan ensures that the transition from the legacy Perl/CGI system to Django is smooth and safe, while providing immediate benefits in terms of a modern UI, better scalability, and maintainable code structure. The proposed architecture is robust yet flexible: it addresses current requirements (reservation workflow, machine allocations, role-specific views, security) and sets the stage for future enhancements (like a React front-end or deeper integrations). We’ve prioritized security and performance throughout, using proven technologies and best practices at each layer. With this approach, ASML’s test reservation and allocation system will be well-equipped to support its users efficiently and securely, now and in the future.

